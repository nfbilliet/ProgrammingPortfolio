%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{2}



\title{GQCML Documentation}
\date{Apr 12, 2021}
\release{0.1}
\author{Niels Billiet}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{gqcml.data\_generators package}
\label{\detokenize{modules/gqcml.data_generators:gqcml-data-generators-package}}\label{\detokenize{modules/gqcml.data_generators::doc}}

\section{Submodules}
\label{\detokenize{modules/gqcml.data_generators:submodules}}

\section{gqcml.data\_generators.Huckel module}
\label{\detokenize{modules/gqcml.data_generators:module-gqcml.data_generators.Huckel}}\label{\detokenize{modules/gqcml.data_generators:gqcml-data-generators-huckel-module}}\index{module@\spxentry{module}!gqcml.data\_generators.Huckel@\spxentry{gqcml.data\_generators.Huckel}}\index{gqcml.data\_generators.Huckel@\spxentry{gqcml.data\_generators.Huckel}!module@\spxentry{module}}\index{HuckelSolver (class in gqcml.data\_generators.Huckel)@\spxentry{HuckelSolver}\spxextra{class in gqcml.data\_generators.Huckel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.Huckel.HuckelSolver}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.data\_generators.Huckel.}}\sphinxbfcode{\sphinxupquote{HuckelSolver}}}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{ONV() (gqcml.data\_generators.Huckel.HuckelSolver method)@\spxentry{ONV()}\spxextra{gqcml.data\_generators.Huckel.HuckelSolver method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.Huckel.HuckelSolver.ONV}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{ONV}}}{\emph{\DUrole{n}{eigenvals}}, \emph{\DUrole{n}{N}}, \emph{\DUrole{n}{N\_a}}, \emph{\DUrole{n}{N\_b}}}{}
Constructs the occupation number vectors that determines the filling of the electrons in the
spin orbitals. This functions takes degenerate states into account as well

Arguments
\begin{quote}
\begin{quote}\begin{description}
\item[{param eigenvals (np.array)}] \leavevmode
A 1D array containing the eigenvalues of the system

\item[{param N (int)}] \leavevmode
The amount of spin orbitals in the system, i.e. the maximum number
of electrons in the ONV

\item[{param .. math:}] \leavevmode
N\_a (int): The amount of .. math:: lpha electrons in the system

\item[{param .. math:}] \leavevmode
N\_b (int): The amount of .. math:: beta electrons in the system

\item[{return ONV (np.array)}] \leavevmode
The occupation number vector for both .. math:: lpha and .. math:: eta are updated

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}

\index{compute\_density\_matrix() (gqcml.data\_generators.Huckel.HuckelSolver method)@\spxentry{compute\_density\_matrix()}\spxextra{gqcml.data\_generators.Huckel.HuckelSolver method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.Huckel.HuckelSolver.compute_density_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{compute\_density\_matrix}}}{\emph{\DUrole{n}{N\_a}}, \emph{\DUrole{n}{N\_b}}}{}
Computes the groundstate density of based on the ONV

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{math:}} (\sphinxstyleliteralemphasis{\sphinxupquote{.}}) \textendash{} N\_a (int): The number of .. math:: lpha electrons

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{math:}} \textendash{} N\_b (int): The number of .. math:: eta electrons

\end{itemize}

\item[{Return P (np.array)}] \leavevmode
A (N x N) numpy array that represents the total electron density matrix from the system
when it contains .. math:: N\_a and .. math:: N\_b electrons

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_energy() (gqcml.data\_generators.Huckel.HuckelSolver method)@\spxentry{compute\_energy()}\spxextra{gqcml.data\_generators.Huckel.HuckelSolver method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.Huckel.HuckelSolver.compute_energy}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{compute\_energy}}}{\emph{\DUrole{n}{N\_a}}, \emph{\DUrole{n}{N\_b}}}{}
Computes the molecular energy in the Huckel model based on the occupation of
alpha and beta electrons of the molecular orbitals. This functions assumes that
the lowest eigenvalues are the first elements

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{math:}} (\sphinxstyleliteralemphasis{\sphinxupquote{.}}) \textendash{} N\_a (int): The number of .. math:: lpha electrons

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{math:}} \textendash{} N\_b (int): The number of .. math:: eta electrons

\end{itemize}

\item[{Return E (float)}] \leavevmode
A floating point value that corresponds to the total energy of the system
when contain .. math:: N\_a and .. math:: N\_b electrons

\end{description}\end{quote}

\end{fulllineitems}

\index{solve\_general() (gqcml.data\_generators.Huckel.HuckelSolver method)@\spxentry{solve\_general()}\spxextra{gqcml.data\_generators.Huckel.HuckelSolver method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.Huckel.HuckelSolver.solve_general}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve\_general}}}{\emph{\DUrole{n}{H}}, \emph{\DUrole{n}{S}}}{}
A function that solves the Schrödinger equation working when including a general overlap

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{S}}) \textendash{} A (N x N) numpy array that represents the system that needs to be computed

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A (N x N) numpy array that contains the overlap values associated with the
given H

\end{itemize}

\item[{Return None}] \leavevmode
The function does computes the eigenvalues and eigenvectors of the given matrix H
using the eigh function from scipy. These solutions are stored in the corresponding attributes

\end{description}\end{quote}

\end{fulllineitems}

\index{solve\_ndo() (gqcml.data\_generators.Huckel.HuckelSolver method)@\spxentry{solve\_ndo()}\spxextra{gqcml.data\_generators.Huckel.HuckelSolver method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.Huckel.HuckelSolver.solve_ndo}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve\_ndo}}}{\emph{\DUrole{n}{H}}}{}
A function that solves the Schrödinger equation working under the assumption of non\sphinxhyphen{}differential overlap

Parameters
:param H (np.array): A (N x N) numpy array that represents the system that needs to be computed
:return None: The function does computes the eigenvalues and eigenvectors of the given matrix H
\begin{quote}

using the eigh function from numpy. These solutions are stored in the corresponding attributes
\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.data\_generators.graph\_sampler module}
\label{\detokenize{modules/gqcml.data_generators:module-gqcml.data_generators.graph_sampler}}\label{\detokenize{modules/gqcml.data_generators:gqcml-data-generators-graph-sampler-module}}\index{module@\spxentry{module}!gqcml.data\_generators.graph\_sampler@\spxentry{gqcml.data\_generators.graph\_sampler}}\index{gqcml.data\_generators.graph\_sampler@\spxentry{gqcml.data\_generators.graph\_sampler}!module@\spxentry{module}}\index{graph\_sampler (class in gqcml.data\_generators.graph\_sampler)@\spxentry{graph\_sampler}\spxextra{class in gqcml.data\_generators.graph\_sampler}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.graph_sampler.graph_sampler}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.data\_generators.graph\_sampler.}}\sphinxbfcode{\sphinxupquote{graph\_sampler}}}{\emph{\DUrole{n}{sites}}}{}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{convert\_triu\_to\_mat() (gqcml.data\_generators.graph\_sampler.graph\_sampler method)@\spxentry{convert\_triu\_to\_mat()}\spxextra{gqcml.data\_generators.graph\_sampler.graph\_sampler method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.graph_sampler.graph_sampler.convert_triu_to_mat}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{convert\_triu\_to\_mat}}}{\emph{\DUrole{n}{triu\_vector}}}{}
\end{fulllineitems}

\index{generate\_diagonal\_vector() (gqcml.data\_generators.graph\_sampler.graph\_sampler method)@\spxentry{generate\_diagonal\_vector()}\spxextra{gqcml.data\_generators.graph\_sampler.graph\_sampler method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.graph_sampler.graph_sampler.generate_diagonal_vector}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_diagonal\_vector}}}{\emph{\DUrole{n}{nmb\_el\_type}}}{}
A function that generates a set of vectors that specifies all possible combinations of the diagonal elements of the adjacency matrix.

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{nmb\_el\_type}}) \textendash{} A list that contains the number of occurence of each unique element,
e.g. suppose we have a diagonal of length 4 with 2 unique elements in equal occurence then this would
correspond with {[}2,2{]}

\item[{Returns}] \leavevmode
A set of vectors that contain all possible permutations of a vector that has the
specified ratios of these elements. E.g. for 2 unique elements in a diagonal of length 4 (equal ratios)
this would return {[}{[}1,1,2,2{]},{[}1,2,1,2{]}, {[}2,1,1,2{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{permutate\_matrix() (gqcml.data\_generators.graph\_sampler.graph\_sampler method)@\spxentry{permutate\_matrix()}\spxextra{gqcml.data\_generators.graph\_sampler.graph\_sampler method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.graph_sampler.graph_sampler.permutate_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{permutate\_matrix}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{permutations}}}{}
\end{fulllineitems}

\index{sample\_homogeneous\_matrix() (gqcml.data\_generators.graph\_sampler.graph\_sampler method)@\spxentry{sample\_homogeneous\_matrix()}\spxextra{gqcml.data\_generators.graph\_sampler.graph\_sampler method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.graph_sampler.graph_sampler.sample_homogeneous_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{sample\_homogeneous\_matrix}}}{\emph{\DUrole{n}{triu\_vector}}, \emph{\DUrole{n}{amount\_samples}}, \emph{\DUrole{n}{diagonal\_interval}\DUrole{o}{=}\DUrole{default_value}{{[}\sphinxhyphen{} 5, 0.001{]}}}, \emph{\DUrole{n}{off\_diagonal\_interval}\DUrole{o}{=}\DUrole{default_value}{{[}\sphinxhyphen{} 5, \sphinxhyphen{} 0.001{]}}}}{}
A function that takes a upper triangle of a homogeneous systems, i.e. a graph consisting of a single
vertex type, and generates an amount of samples within the specified sampling ranges

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{triu\_vector}}) \textendash{} A upper triangle np array that describes the structure of the system
The array consists of 0’s and 1’s where the 1 signifies a weight.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{amount\_samples}}) \textendash{} An integer that determines how many samples need to be generated

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{opt}}\sphinxstyleliteralstrong{\sphinxupquote{, }}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{off\_diagonal\_interval}}) \textendash{} A list of floats that determines the lower and upper range
of the uniform random distribution used in the generation of the samples
for the diagonal elements of the adjacency matrix

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{opt}}\sphinxstyleliteralstrong{\sphinxupquote{, }}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A list of floats that determines the lower and upper range
of the uniform random distribution used in the generation of the samples
for the off\sphinxhyphen{}diagonal elements of the adjacency matrix

\end{itemize}

\item[{Return sampled\_trius (list)}] \leavevmode
A list of numpy arrays containing the upper triangle vector sampled randomly

\end{description}\end{quote}

\end{fulllineitems}

\index{sample\_inhomogeneous\_matrix() (gqcml.data\_generators.graph\_sampler.graph\_sampler method)@\spxentry{sample\_inhomogeneous\_matrix()}\spxextra{gqcml.data\_generators.graph\_sampler.graph\_sampler method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data_generators:gqcml.data_generators.graph_sampler.graph_sampler.sample_inhomogeneous_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{sample\_inhomogeneous\_matrix}}}{\emph{\DUrole{n}{diagonal\_vector}}, \emph{\DUrole{n}{triu\_vector}}, \emph{\DUrole{n}{amount\_samples}}, \emph{\DUrole{n}{diagonal\_interval}\DUrole{o}{=}\DUrole{default_value}{{[}\sphinxhyphen{} 5, 0.001{]}}}, \emph{\DUrole{n}{off\_diagonal\_interval}\DUrole{o}{=}\DUrole{default_value}{{[}\sphinxhyphen{} 5, \sphinxhyphen{} 0.001{]}}}}{}~\begin{description}
\item[{Constructs a weighted adjacency matrix sampled uniformly based on the symbolic assignment of the elements}] \leavevmode\begin{itemize}
\item {} 
The diagonal elements are assigned symbolic through integers. These integers will be replaced by random values

\item {} 
Off diagonal elements are sampled based on the the diagonal indices

\end{itemize}

\end{description}

This function will thus generate an adjacency matrix that has unique weights for every type of bond.
A type of bond is considered to be the edge between a pair of diagonal weihgts.

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{triu\_vector}}) \textendash{} A np array of integers that symbolic denotes the type of vertex.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A upper triangle np array that describes the structure of the system
The array consists of 0’s and 1’s where the 1 signifies a weight.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{amount\_samples}}) \textendash{} An integer that determines how many samples need to be generated

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{opt}}\sphinxstyleliteralstrong{\sphinxupquote{, }}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{off\_diagonal\_interval}}) \textendash{} A list of floats that determines the lower and upper range
of the uniform random distribution used in the generation of the samples
for the diagonal elements of the adjacency matrix

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{opt}}\sphinxstyleliteralstrong{\sphinxupquote{, }}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A list of floats that determines the lower and upper range
of the uniform random distribution used in the generation of the samples
for the off\sphinxhyphen{}diagonal elements of the adjacency matrix

\end{itemize}

\item[{Return sampled\_trius (list)}] \leavevmode
A list of numpy arrays containing the upper triangle vector sampled randomly

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Module contents}
\label{\detokenize{modules/gqcml.data_generators:module-gqcml.data_generators}}\label{\detokenize{modules/gqcml.data_generators:module-contents}}\index{module@\spxentry{module}!gqcml.data\_generators@\spxentry{gqcml.data\_generators}}\index{gqcml.data\_generators@\spxentry{gqcml.data\_generators}!module@\spxentry{module}}

\chapter{gqcml.data package}
\label{\detokenize{modules/gqcml.data:gqcml-data-package}}\label{\detokenize{modules/gqcml.data::doc}}

\section{Submodules}
\label{\detokenize{modules/gqcml.data:submodules}}

\section{gqcml.data.Data module}
\label{\detokenize{modules/gqcml.data:module-gqcml.data.Data}}\label{\detokenize{modules/gqcml.data:gqcml-data-data-module}}\index{module@\spxentry{module}!gqcml.data.Data@\spxentry{gqcml.data.Data}}\index{gqcml.data.Data@\spxentry{gqcml.data.Data}!module@\spxentry{module}}\index{Preprocessor (class in gqcml.data.Data)@\spxentry{Preprocessor}\spxextra{class in gqcml.data.Data}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.data.Data.}}\sphinxbfcode{\sphinxupquote{Preprocessor}}}{\emph{\DUrole{n}{graph\_dim}}}{}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{adjacency\_matrix() (gqcml.data.Data.Preprocessor method)@\spxentry{adjacency\_matrix()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.adjacency_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{adjacency\_matrix}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{diagonal}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{negative\_weights}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
A function to format the matrix into a adjacency matrix format.
The function has the capability to change the diagonal elements of the matrix
according to the argument given to ‘diagonal’

In addition to this we also provide the option to normalize the given adjacency matrix
using the following formula
\begin{equation*}
\begin{split}Ã=D^{-1/2}AD^{-1/2}\end{split}
\end{equation*}
where the normalized is computed using the absolute value of the adjacency matrix

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{matrix}}) \textendash{} A numpy array that represents the graph that is currently being processed
The array has the dimension (N x N) where N is the number of vertices in the graph

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{diagonal}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{str}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} 
An optional parameter that determines the diagonal of this matrix. Standardly
we set this value to None resulting in a diagonal that remains unchanged.
Alternatively we can give the following keywords
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
’ones’\sphinxhyphen{}replace the diagonal elements with ones

\item {} 
’zeros’\sphinxhyphen{}replace the diagonal elements with zero

\end{enumerate}


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{normalize}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} A boolean that determines whether the matrix should be normalized using its
degree matrix (computed using the absolute values of the matrix)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{negative\_weights}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} A boolean that is used in the determination of the added one to the
diagonal (negative or positive) and how the normalization is
computed

\end{itemize}

\item[{Return adjacency\_matrix (np.array)}] \leavevmode
The adjacency matrix

\end{description}\end{quote}

\end{fulllineitems}

\index{binarize\_matrix() (gqcml.data.Data.Preprocessor method)@\spxentry{binarize\_matrix()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.binarize_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{binarize\_matrix}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{diagonal}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
A function that binarizes a given matrix with the option to exclude the diagonal elements
Parameters
:param matrix (np.array): A 2D numpy array that will be converted to its binary form
:param (opt) diagonal (bool): A boolean that indicates whether the diagonal should be 1 or 0
:return binary\_matrix (np.array): A binary representation of the input matrix

\end{fulllineitems}

\index{pdegree\_weighted\_nf() (gqcml.data.Data.Preprocessor method)@\spxentry{pdegree\_weighted\_nf()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.pdegree_weighted_nf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{pdegree\_weighted\_nf}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{num\_degrees}}}{}
Constructs a seperate class representation that characterizes the edges that
connect to the central vertex. This class representation is constructed as a set of tuples
where the first element of the tuple describes the central vertex’s degree and the second element
describes the connecting vertex’s degree.

\{(1,1), (1,2),…, (2,1), (2,2), …\}.

as such this class degree will have the dimension (1 X .. math::M\textasciicircum{}2) where M is the predefined maximum degree number
(standarly this will be set to 3 when working with organic molecules).We provide the option to process the
following weight types
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
“diagonal”, the diagonal elements of the matrix will be used in the neighbourhood description
of the central vertex.

\item {} 
“off diagonal”, the off diagonal elements of the matrix will be used in the neighbourhood description
of the central vertex

\end{enumerate}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{numpy array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{matrix}}) \textendash{} A 2D matrix (N X N) representing the graph that is to be processed. This matrix
is a square matrix that contains the weights that will be used in weighting
the generated categorical vectors. The weights can either be the edge weights
or the self loop weights (diagonal repeated as a row).

\end{description}\end{quote}

\end{fulllineitems}

\index{triu\_to\_matrix() (gqcml.data.Data.Preprocessor method)@\spxentry{triu\_to\_matrix()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.triu_to_matrix}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{triu\_to\_matrix}}}{\emph{\DUrole{n}{triu}}}{}
Convert a vector of upper triangular values to a full, symmetric matrix.

Parameters
:param triu (np.array): A 1D numpy array containing the upper triangle values of a symmetric
\begin{quote}

matrix
\end{quote}
\begin{quote}\begin{description}
\item[{Return matrix (np.array)}] \leavevmode
The matrix form of the upper triangle

\end{description}\end{quote}

\end{fulllineitems}

\index{vdegree\_nf() (gqcml.data.Data.Preprocessor method)@\spxentry{vdegree\_nf()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.vdegree_nf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{vdegree\_nf}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{categorical}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{num\_degrees}\DUrole{o}{=}\DUrole{default_value}{3}}}{}
A funtion that formats the weighted matrix to generate a vertex degree feature based on the
weights in the matrix. The degree is computed as the number of weights that each vertex possesses.
In addition  we standardly enable the conversion of this integer feature to the
categorical form.

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{numpy array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{matrix}}) \textendash{} A numpy array that represents the adjacency matrix that represents the
graph.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{categorical}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Option to convert the degree feature into the categorical format.
Standard value for this argument is True

\end{itemize}

\item[{Return vdegree\_nf, matrix (np.array)}] \leavevmode
A numpy array that describes the vertices in terms of their degree

\end{description}\end{quote}

\end{fulllineitems}

\index{vdegree\_weighted\_nf() (gqcml.data.Data.Preprocessor method)@\spxentry{vdegree\_weighted\_nf()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.vdegree_weighted_nf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{vdegree\_weighted\_nf}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{neighbourhood}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{weight\_method}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}average\textquotesingle{}}}}{}
A vertex degree node feature constructor that weights the categorical vector
using different weights types. The weight types that we specify are the following options

If “neighbourhood” is set to true we weight the categorical with the self loops but also weights 2 additional
categorical vectors. These 2 vectors take the following weighting into account
\begin{itemize}
\item {} 
The edges weights

\item {} 
The self loops of the connecting vertices\textasciigrave{}

\end{itemize}

When using the neighbourhood weight type we als can specify how to weight these neighbourhood
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
“average”, the off diagonal elements are average and subsequently used to weight
the degree vector of the central vertex

\item {} 
“linear combination”, the off diagonal elements are weighted using the degree vector
of the connecting vertices which are then subsequently summed and averaged per
degree class

\end{enumerate}

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{numpy array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{matrix}}) \textendash{} A 2D matrix (N X N) representing the graph that is to be processed. This matrix
is a square matrix that contains the weights that will be used in weighting
the generated categorical vectors. The weights can either be the edge weights
or the self loop weights (diagonal repeated as a row)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{neighbourhood}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Boolean that enables the option to include information about the neighbourhood
weights. The edge weights and self loop weights of the neighbouring vertices
are formatted in the categorical format. The method which this is done is
determined by the weight\_method optional argument

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{weight\_method}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{str}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} 
Method by which we incorporate neighbourhood information into the node feature
representation.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
”average”, neighbourhood weights are averaged and then used to weight the
the degree vector of the central vertex

\item {} 
”linear combination” neighbourhood weights are weighted with the degree vectors
of the connecting vertex and subsequently summed up and averaged, meaning that
we count the amount of neighbouring vertices have a specific degree which will be
used to rescale the summed combination

\end{enumerate}


\end{itemize}

\item[{Return vdegree\_weighted\_nf (numpy.array)}] \leavevmode
a numpy array that describes the vertices as a categorical degree vector weighted
by its self loop weight

\end{description}\end{quote}

\end{fulllineitems}

\index{weights\_nf() (gqcml.data.Data.Preprocessor method)@\spxentry{weights\_nf()}\spxextra{gqcml.data.Data.Preprocessor method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.data:gqcml.data.Data.Preprocessor.weights_nf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{weights\_nf}}}{\emph{\DUrole{n}{matrix}}, \emph{\DUrole{n}{edge\_weights}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
A function that formats the weighted matrix as node\_features using the self loop weight
as the node features. In addition to this we also provide to option to include the average
of the edge weights as an additional feature

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{numpy array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{matrix}}) \textendash{} A 2D numpy array that contains diagonal and off\sphinxhyphen{}diagonal weights
that parametrize the graph

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{edge\_weights}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} A boolean argument that determines whether the average of the off\sphinxhyphen{}diagonal
elements should be included as an additional node feature

\end{itemize}

\item[{Return node\_features}] \leavevmode
Numpy array where the node features are the self loop
weights and possibly the average of the edge weights as well

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Module contents}
\label{\detokenize{modules/gqcml.data:module-gqcml.data}}\label{\detokenize{modules/gqcml.data:module-contents}}\index{module@\spxentry{module}!gqcml.data@\spxentry{gqcml.data}}\index{gqcml.data@\spxentry{gqcml.data}!module@\spxentry{module}}

\chapter{gqcml.datasets package}
\label{\detokenize{modules/gqcml.datasets:gqcml-datasets-package}}\label{\detokenize{modules/gqcml.datasets::doc}}

\section{Submodules}
\label{\detokenize{modules/gqcml.datasets:submodules}}

\section{gqcml.datasets.Datasets module}
\label{\detokenize{modules/gqcml.datasets:module-gqcml.datasets.Datasets}}\label{\detokenize{modules/gqcml.datasets:gqcml-datasets-datasets-module}}\index{module@\spxentry{module}!gqcml.datasets.Datasets@\spxentry{gqcml.datasets.Datasets}}\index{gqcml.datasets.Datasets@\spxentry{gqcml.datasets.Datasets}!module@\spxentry{module}}\index{DataLoader\_constructor() (in module gqcml.datasets.Datasets)@\spxentry{DataLoader\_constructor()}\spxextra{in module gqcml.datasets.Datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.datasets:gqcml.datasets.Datasets.DataLoader_constructor}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.datasets.Datasets.}}\sphinxbfcode{\sphinxupquote{DataLoader\_constructor}}}{\emph{\DUrole{n}{input\_tensors}}, \emph{\DUrole{n}{output\_tensors}}, \emph{\DUrole{n}{batch\_size}}, \emph{\DUrole{n}{shuffle}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{pin\_memory}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{num\_workers}\DUrole{o}{=}\DUrole{default_value}{0}}}{}
A function that takes in a set of input tensors and a set of output tensor and
constructs a data loader that can be used in the optimization of network.

Parameters
:param input\_tensors (list of torch.Tensor): A list of input tensors
:param output\_tensors (torch.Tensor): A tensor that contains the corresponding output tensors
:param batch\_size (int): An integer that determines the number of tensors in each batch
:param shuffle (opt, bool): A boolean that enables the option to shuffle the data during batch iteration
:param pin\_memory (opt, bool): A boolean that enables the option to pin memory in the GPUs utilized during training.
\begin{quote}

This allows for faster data transfer
\end{quote}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{opt}}\sphinxstyleliteralstrong{\sphinxupquote{, }}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{num\_workers}}) \textendash{} An integer that controls how many subprocesses to use for data loading.
0 means that the data will be loaded in the main process

\item[{Return loader (torch.data.DataLoader)}] \leavevmode
A DataLoader object that returns batch objects

\end{description}\end{quote}

\end{fulllineitems}

\index{bin\_values() (in module gqcml.datasets.Datasets)@\spxentry{bin\_values()}\spxextra{in module gqcml.datasets.Datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.datasets:gqcml.datasets.Datasets.bin_values}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.datasets.Datasets.}}\sphinxbfcode{\sphinxupquote{bin\_values}}}{\emph{\DUrole{n}{target\_values}}, \emph{\DUrole{n}{num\_bins}}}{}
A function to partition a continuous target variable into discrete bins. This function is intended
to work together with the train\_test\_split functionality from sklearn.model\_selection to partition the
dataset into representative training, validation and test data by ensuring that target values are equally
represented
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{target\_values}}) \textendash{} A 2D numpy array containing the continuous target values of the dataset.
Each row represents the target value of a datapoint

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{num\_bins}}) \textendash{} An integer that determines the number of output classes that the function returns

\end{itemize}

\item[{Return binned\_values (np.array)}] \leavevmode
A 2D array where the values of the input array have been classified into
discrete bins determined by the minimum and maximum of the input array.

\end{description}\end{quote}

\end{fulllineitems}

\index{split\_dataset() (in module gqcml.datasets.Datasets)@\spxentry{split\_dataset()}\spxextra{in module gqcml.datasets.Datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.datasets:gqcml.datasets.Datasets.split_dataset}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.datasets.Datasets.}}\sphinxbfcode{\sphinxupquote{split\_dataset}}}{\emph{\DUrole{n}{input\_values}}, \emph{\DUrole{n}{target\_values}}, \emph{\DUrole{n}{data\_frac}}, \emph{\DUrole{n}{seed}}, \emph{\DUrole{n}{stratify}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{num\_bins}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{shuffle}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Split\_dataset

A function that splits a dataset into a training, validation and test set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{output\_values}}) \textendash{} A 2D numpy array containing the input values of the dataset. Each row represents an input vector of the dataset.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A 2D numpy array containing the continuous target values of the dataset.
Each row represents the target value of a datapoint.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{float}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{data\_frac}}) \textendash{} The fraction of the dataset that should be reserved for the validation and test set.
This fraction will be split in half for both these sets.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{seed}}) \textendash{} The random seed used to determine the split of the dataset.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{stratify}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Option to partition the dataset according to the distribution of the target values. This option
ensures that all the sets follow the same distribution in their target.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_bins}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} An integer that determines the number of output classes

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{shuffle}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Option to shuffle the dataset before partitioning it into a training, validation and test set.

\end{itemize}

\item[{Return inputs, outputs (tuple)}] \leavevmode
The function returns a tuple of the split dataset in the order of input data followed by output data.
The order in which this happens is training, validation and test set

\end{description}\end{quote}

\end{fulllineitems}

\index{trius\_to\_inputs() (in module gqcml.datasets.Datasets)@\spxentry{trius\_to\_inputs()}\spxextra{in module gqcml.datasets.Datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.datasets:gqcml.datasets.Datasets.trius_to_inputs}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.datasets.Datasets.}}\sphinxbfcode{\sphinxupquote{trius\_to\_inputs}}}{\emph{\DUrole{n}{trius}}, \emph{\DUrole{n}{preprocessor}}, \emph{\DUrole{n}{am\_args}}, \emph{\DUrole{n}{preprocessor\_nf\_method}}, \emph{\DUrole{n}{nf\_args}}}{}
trius\_to\_input

A function that interacts with the preprocessor class defined in the Data section of gqcml.
The function takes as stack of upper triangle values and converts them to their matrix representation.
Following the conversion to matrices we utilize methods defined in the preprocessor class to
convert the matrices to a suitable input for graph neural networks. The different methods that are
defined are
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
(categorical) degree node feature

\item {} 
weight node feature

\item {} 
weighted categorical degree

\item {} 
weighted categorical pair degree

\end{enumerate}

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{np.array}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{trius}}) \textendash{} A numpy array containing the trius of the graphs that are to be processed. This
array has the dimension (K x M) where K is the number of graphs contained in the
dataset and M=N(N\sphinxhyphen{}1)/2 with N the number of vertices in the graph

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{gqcml.data.Data}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{preprocessor\_am\_method}}) \textendash{} A function from the Preprocessor class that processes
the matrices to the adjacency matrices

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{args}}) \textendash{} The list of function arguments (excluding the input matrix) for the am\_method.
When no arguments need to specified the input should be an empty list

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{gqcml.data.Data.Preprocessor}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{preprocessor\_nf\_method}}) \textendash{} A function from the Preprocessor class that
processes the matrices into the desired node
feature class

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} The list of function arguments (excluding the input matrix) for the nf\_method.
When no arguments need to specified the input should be an empty list

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{list}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} The arguments for the preprocessor\_method excluding the matrix to be processed.

\end{itemize}

\item[{Return tuple of numpy arrays}] \leavevmode
Returns the processed node features and the adjacency matrices

\end{description}\end{quote}

\end{fulllineitems}



\section{Module contents}
\label{\detokenize{modules/gqcml.datasets:module-gqcml.datasets}}\label{\detokenize{modules/gqcml.datasets:module-contents}}\index{module@\spxentry{module}!gqcml.datasets@\spxentry{gqcml.datasets}}\index{gqcml.datasets@\spxentry{gqcml.datasets}!module@\spxentry{module}}

\chapter{gqcml.nn package}
\label{\detokenize{modules/gqcml.nn:gqcml-nn-package}}\label{\detokenize{modules/gqcml.nn::doc}}

\section{Submodules}
\label{\detokenize{modules/gqcml.nn:submodules}}

\section{gqcml.nn.blocks module}
\label{\detokenize{modules/gqcml.nn:module-gqcml.nn.blocks}}\label{\detokenize{modules/gqcml.nn:gqcml-nn-blocks-module}}\index{module@\spxentry{module}!gqcml.nn.blocks@\spxentry{gqcml.nn.blocks}}\index{gqcml.nn.blocks@\spxentry{gqcml.nn.blocks}!module@\spxentry{module}}\index{GraphConv\_Block (class in gqcml.nn.blocks)@\spxentry{GraphConv\_Block}\spxextra{class in gqcml.nn.blocks}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.blocks.GraphConv_Block}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.blocks.}}\sphinxbfcode{\sphinxupquote{GraphConv\_Block}}}{\emph{\DUrole{n}{node\_input\_dim}}, \emph{\DUrole{n}{nmb\_GC\_layers}}, \emph{\DUrole{n}{nmb\_filter\_dlayers}}, \emph{\DUrole{n}{activation\_function}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

A sequential block of GraphConv operators that can be used a building module of a graph convolutional model.

Attributes
:var node\_input\_dim (int): The number of features that are associated with each node in the graph G
:var nmb\_GraphConv\_layers (int): The number of GraphConv layers that need to be present in the block
:var nmb\_filter\_dlayers (int): The number of non\sphinxhyphen{}linear transformations that are performed in each graph convolution
\begin{quote}

after aggregation of the neighbourhood features. The dense layers standardly are constructed
to output as many amount of features that go into them
\end{quote}
\index{forward() (gqcml.nn.blocks.GraphConv\_Block method)@\spxentry{forward()}\spxextra{gqcml.nn.blocks.GraphConv\_Block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.blocks.GraphConv_Block.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{node\_feat}}, \emph{\DUrole{n}{adj\_matrix}}}{}
Forward propagation function that produces an output

Parameters
:param node\_feat (torch.Tensor): The initial node feature tensors that are associated with a batch of graphs. This
\begin{quote}

tensor should have the dimensions (B x N x F) where B is the batch size, N the
number of nodes present in the graphs and F the number of features associated
with each node present in the graph
\end{quote}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{adj\_matrix}}) \textendash{} The weighted adjacency tensors that are associated with a batch of graphs. This
tensor should have the dimensions (B x N x N) where B is the batch size and N
the number of nodes present in the graph

\item[{Return transf\_node\_feat (torch.Tensor)}] \leavevmode
The transformed node features that are obtained by consecutively performing
the graph convolutional operator. This tensor has the dimensions (B x N x F)
where B is the batch size, N the number of nodes present in the graph and
F is the number of features associated with each node

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.nn.layers module}
\label{\detokenize{modules/gqcml.nn:module-gqcml.nn.layers}}\label{\detokenize{modules/gqcml.nn:gqcml-nn-layers-module}}\index{module@\spxentry{module}!gqcml.nn.layers@\spxentry{gqcml.nn.layers}}\index{gqcml.nn.layers@\spxentry{gqcml.nn.layers}!module@\spxentry{module}}\index{GaussianEmbedding (class in gqcml.nn.layers)@\spxentry{GaussianEmbedding}\spxextra{class in gqcml.nn.layers}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GaussianEmbedding}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.layers.}}\sphinxbfcode{\sphinxupquote{GaussianEmbedding}}}{\emph{\DUrole{n}{lower}}, \emph{\DUrole{n}{upper}}, \emph{\DUrole{n}{num\_gaussians}}, \emph{\DUrole{n}{emb\_dim}}, \emph{\DUrole{n}{variance}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{emb\_bias}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

A function that combines the GaussianExpansion layer with the concept of embedding.
Should the initial features be described by a single continuous variable the problem
becomes that learning an embedding for this becomes difficult due to the mathematical
construct of embedding where
\begin{equation*}
\begin{split}EMB(v_i) = Zv_i\end{split}
\end{equation*}
Where Z is a learnable set of weights. In the case of a single node feature v\_i
the dimension of this Z becomes a vector and thus does not contain many learnable parameters.
When we use the Gaussian expansion layer we convert this continuous variable to “quasi” binned variable
where our bins consist of the different gaussians within the layer and the layer measures “overlap” with the gaussians.
Using this expanded representation we provide more freedom to the Z matrix in our embedding due to the increased dimensionality.

Attributes
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{float}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{variance}}) \textendash{} A float that bounds the linear space from which the means are generated.
The space spans the interval {[}lower, upper{]}

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{float}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A float that bounds the linear space from which the means are generated.
The space spans the interval {[}lower, upper{]}

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{float}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} A float that controls the width of gaussians that we construct.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{emb\_dim}}) \textendash{} The number of gaussian functions that we place within the linear space.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} The dimension of the embedded node feature.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{emb\_bias}}\sphinxstyleliteralstrong{\sphinxupquote{ (}}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{opt}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} A boolean that controls the addition of a learnable bias for the embedding

\end{itemize}

\end{description}\end{quote}
\index{forward() (gqcml.nn.layers.GaussianEmbedding method)@\spxentry{forward()}\spxextra{gqcml.nn.layers.GaussianEmbedding method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GaussianEmbedding.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{inp}}}{}
A function that calls the forward propagation

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}} (\sphinxstyleliteralemphasis{\sphinxupquote{inp}}) \textendash{} A (B x N x 1) tensor, where B is the batch size and N is the number of nodes,
that should be expaned in a Gaussian basis

\item[{Return embedded tensor (torch.Tensor)}] \leavevmode
A (B x N x E) tensor where E is the embedding dimension

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{GaussianExpansion (class in gqcml.nn.layers)@\spxentry{GaussianExpansion}\spxextra{class in gqcml.nn.layers}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GaussianExpansion}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.layers.}}\sphinxbfcode{\sphinxupquote{GaussianExpansion}}}{\emph{\DUrole{n}{lower}}, \emph{\DUrole{n}{upper}}, \emph{\DUrole{n}{num\_gaussians}}, \emph{\DUrole{n}{variance}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\begin{description}
\item[{A layer that takes an input tensor and expands it into a gaussian basis.}] \leavevmode
The layer takes a tensor consisting of single descriptors and bins this descriptor.
The descriptor is fed to a function that computes the output value of a series of gaussians
that are equidistantly spaced using a linear space of means and a single variance parameter.
The output of this function thus maps a scalar value to a vector with the dimension equal to the
number of gaussians that we have chosen to span the linear space of means.
.. math:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZbs{}\PYG{n}{phi}\PYG{p}{(}
\end{sphinxVerbatim}

\end{description}

u\_i)=
u\_i
ightarrow
ec\{
u\_i\}: mathbb\{R\} mapsto mathbb\{R\}\textasciicircum{}N
\begin{quote}

phi(
\end{quote}

u\_i)=egin\{bmatrix\} e\textasciicircum{}\{
u\_i\sphinxhyphen{}mu\_1\}\{2sigma\textasciicircum{}2\} \& e\textasciicircum{}\{
u\_i\sphinxhyphen{}mu\_2\}\{2sigma\textasciicircum{}2\} \& dots e\textasciicircum{}\{
u\_i\sphinxhyphen{}mu\_N\}\{2sigma\textasciicircum{}2\} end\{bmatrix\}
\begin{quote}

Attributes
:var lower (float): A float that bounds the linear space from which the means are generated.
\begin{quote}

The space spans the interval {[}lower, upper{]}
\end{quote}
\begin{quote}\begin{description}
\item[{var upper (float)}] \leavevmode
A float that bounds the linear space from which the means are generated.
The space spans the interval {[}lower, upper{]}

\item[{var num\_gaussians (int)}] \leavevmode
The number of gaussian functions that we place within the linear space.

\item[{var (opt) variance (float)}] \leavevmode
Standardly this is not defined. When this optional parameter is not
defined we compute the variance so that the FWHM of neighbouring
overlaps. If a float is given the variance as defined will be used.

\end{description}\end{quote}
\end{quote}
\index{forward() (gqcml.nn.layers.GaussianExpansion method)@\spxentry{forward()}\spxextra{gqcml.nn.layers.GaussianExpansion method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GaussianExpansion.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{inp}}}{}
Forward phase of the layer

Parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}} (\sphinxstyleliteralemphasis{\sphinxupquote{inp}}) \textendash{} A (B x N x 1) tensor, where B is the batch size and N is the number of nodes,
that should be expaned in a Gaussian basis

\item[{Return expanded tensor (torch.Tensor)}] \leavevmode
A (B x N x M) tensor where M is the number of Gaussians

\end{description}\end{quote}

\end{fulllineitems}

\index{meta() (gqcml.nn.layers.GaussianExpansion method)@\spxentry{meta()}\spxextra{gqcml.nn.layers.GaussianExpansion method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GaussianExpansion.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
A function that extracts the meta data from the layer

\end{fulllineitems}


\end{fulllineitems}

\index{GraphConv (class in gqcml.nn.layers)@\spxentry{GraphConv}\spxextra{class in gqcml.nn.layers}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GraphConv}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.layers.}}\sphinxbfcode{\sphinxupquote{GraphConv}}}{\emph{\DUrole{n}{node\_input\_dim}}, \emph{\DUrole{n}{node\_output\_dim}}, \emph{\DUrole{n}{activation\_function}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

The standard graph convolution operator defined in the paper Semi\sphinxhyphen{}Supervised Classification with Graph Convolutional
Networks (\sphinxurl{https://arxiv.org/abs/1609.02907}).
.. math:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{=} \PYG{n}{h}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{n\PYGZus{}}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}\PYG{o}{.}\PYG{n}{W\PYGZus{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

Where the current node features are aggregated according to the adjacency matrix A and subsequently weighted by a (set of)
weight matrix/matrices. We include the option for a  residual connection by adding the transformed node features to the current
state

Attributes
:var node\_input\_dim (int): The number of features present on the nodes of the graph G when entering the layer
\begin{quote}\begin{description}
\item[{Variables}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{int}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{node\_output\_dim}}) \textendash{} The number of features present in the layer output

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.nn}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{activation\_function}}) \textendash{} The activation function to be used for the filter layer(s) after message construction

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{opt}}\sphinxstyleliteralstrong{\sphinxupquote{, }}\sphinxstyleliteralstrong{\sphinxupquote{bool}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{bias}}) \textendash{} A boolean that enables or disables the bias term in the filter application. In the standard settings
this value is True

\end{itemize}

\end{description}\end{quote}
\index{forward() (gqcml.nn.layers.GraphConv method)@\spxentry{forward()}\spxextra{gqcml.nn.layers.GraphConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GraphConv.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{node\_feat}}, \emph{\DUrole{n}{adj\_matrix}}}{}
Function that calls the forward pass of the layer

Parameters
:param: node\_feat (torch.Tensor): The current node features, this tensor should have the dimensions (B x N x F) where
\begin{quote}

B is the batch size, N is the number of nodes and F is the number of features
\end{quote}
\begin{quote}\begin{description}
\item[{Param}] \leavevmode
adj\_matrix (torch.Tensor): The adjacency matrices by which the node features need to be aggregated, this tensor
should have the dimensions (B x N x N) where B is the batch size and N the number of nodes

\item[{Returns}] \leavevmode
transf\_node\_emb (torch.Tensor): The updated node features with the aggregated features according to the adjacency
matrix. This tensor has the same dimensions as

\end{description}\end{quote}

\end{fulllineitems}

\index{reset\_parameters() (gqcml.nn.layers.GraphConv method)@\spxentry{reset\_parameters()}\spxextra{gqcml.nn.layers.GraphConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.GraphConv.reset_parameters}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset\_parameters}}}{}{}
A function that initializes the weights and biases of the learnable layer

\end{fulllineitems}


\end{fulllineitems}

\index{ShiftedSoftplus (class in gqcml.nn.layers)@\spxentry{ShiftedSoftplus}\spxextra{class in gqcml.nn.layers}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.ShiftedSoftplus}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.layers.}}\sphinxbfcode{\sphinxupquote{ShiftedSoftplus}}}{\emph{\DUrole{n}{beta}\DUrole{o}{=}\DUrole{default_value}{2}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\begin{description}
\item[{The shifted softplus activation function}] \leavevmode\begin{equation*}
\begin{split}ln\left(eta+eta e^{x}\end{split}
\end{equation*}
\end{description}

ight)
\begin{quote}

Attributes
:var beta (opt,float): A shifting factor with standard setting 2
\end{quote}
\index{forward() (gqcml.nn.layers.ShiftedSoftplus method)@\spxentry{forward()}\spxextra{gqcml.nn.layers.ShiftedSoftplus method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.layers.ShiftedSoftplus.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{inp}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.nn.models module}
\label{\detokenize{modules/gqcml.nn:module-gqcml.nn.models}}\label{\detokenize{modules/gqcml.nn:gqcml-nn-models-module}}\index{module@\spxentry{module}!gqcml.nn.models@\spxentry{gqcml.nn.models}}\index{gqcml.nn.models@\spxentry{gqcml.nn.models}!module@\spxentry{module}}\index{DNN (class in gqcml.nn.models)@\spxentry{DNN}\spxextra{class in gqcml.nn.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.DNN}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.models.}}\sphinxbfcode{\sphinxupquote{DNN}}}{\emph{\DUrole{n}{layer\_configs}}, \emph{\DUrole{n}{activation\_function}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

Dense Neural Network

A standard dense network consisting of multiple hidden layers

Arguments
\begin{quote}
\begin{description}
\item[{layer\_dims (list)}] \leavevmode{[}A list containing the number of nodes in each layer for the network.{]}
The first entry in the list corresponds to the number of features in the input tensor
and the last entry in the list corresponds to the number of features in the output tensor

\end{description}

activation\_function (torch.nn): The activation used in the network after each hidden layer
\end{quote}
\index{forward() (gqcml.nn.models.DNN method)@\spxentry{forward()}\spxextra{gqcml.nn.models.DNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.DNN.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{input\_tensor}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{meta() (gqcml.nn.models.DNN method)@\spxentry{meta()}\spxextra{gqcml.nn.models.DNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.DNN.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
Takes the input arguments of the class and formats them in a dictionary format which can be used in
creating an overarching meta file

\end{fulllineitems}


\end{fulllineitems}

\index{GraphConv\_model (class in gqcml.nn.models)@\spxentry{GraphConv\_model}\spxextra{class in gqcml.nn.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.GraphConv_model}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.models.}}\sphinxbfcode{\sphinxupquote{GraphConv\_model}}}{\emph{\DUrole{n}{node\_embedding\_nn}}, \emph{\DUrole{n}{GC\_dimensions}}, \emph{\DUrole{n}{activation\_function}}, \emph{\DUrole{n}{node\_prop\_nn}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

Graph Convolution model

A standard graph convolution model constructor

Arguments
\begin{quote}
\begin{description}
\item[{node\_embedding\_nn (torch.nn.Module): A torch.nn.Module that processes the initial node features. This torch module}] \leavevmode
takes a tensor with dimensions (B x N x f)  and should return a tensor with
dimensions (B x N x F) where B is the batch size, N is the number of nodes in
the graphs, f is the initial number of node features and F is the number of node features
associated with each node in the graph. F should be equal to the node\_input\_dim
parameter of the GraphConv layers.

\item[{GC\_dimensions (list): A list of integers describing the consecutive node embedding dimensions that go into the graph}] \leavevmode
convolution layers and are subsequently returned.

\item[{activation\_function (torch.nn): The activation function used non\sphinxhyphen{}linear transformation of the aggregated neighbourhood}] \leavevmode
information.

\item[{node\_prop\_nn (torch.nn.Module): A torch.nn.Module that processes the transformed node features after the application}] \leavevmode
of the graph convolution operators. The network takes the node features with dimension
(B x N x F) where B is the batch size, N is the number of nodes in the graph and F
the number of features associated with each node in the graphs. The output of the network
is a tensor that has the dimensions (B x N x 1) where the resulting features are
mapped to a single node property.

\end{description}
\end{quote}
\index{forward() (gqcml.nn.models.GraphConv\_model method)@\spxentry{forward()}\spxextra{gqcml.nn.models.GraphConv\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.GraphConv_model.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{node\_feat}}, \emph{\DUrole{n}{adj\_matrix}}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{adj\_matrix}}) \textendash{} The initial node feature tensor with the dimensions (B x N x f) where B is the batch
size, N is the number of nodes in each graph and f is the number of node features
associated with each node in the graphs.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} The weighted adjacency tensor with the dimensions (B x N x N) where B is the batch
size and N is the number of nodes in each graph.

\end{itemize}

\item[{Return (torch.Tensor)}] \leavevmode
The graph property tensor with dimensions (B x 1) where B is the batch
size. The graph property is computed as the sum of the node properties obtained through the
site\_prop\_nn

\end{description}\end{quote}

\end{fulllineitems}

\index{meta() (gqcml.nn.models.GraphConv\_model method)@\spxentry{meta()}\spxextra{gqcml.nn.models.GraphConv\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.GraphConv_model.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
\end{fulllineitems}

\index{site\_properties() (gqcml.nn.models.GraphConv\_model method)@\spxentry{site\_properties()}\spxextra{gqcml.nn.models.GraphConv\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models.GraphConv_model.site_properties}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{site\_properties}}}{\emph{\DUrole{n}{node\_feat}}, \emph{\DUrole{n}{adj\_matrix}}}{}~\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}}\sphinxstyleliteralstrong{\sphinxupquote{)}} (\sphinxstyleliteralemphasis{\sphinxupquote{adj\_matrix}}) \textendash{} The initial node feature tensor with the dimensions (B x N x f) where B is the batch
size, N is the number of nodes in each graph and f is the number of node features
associated with each node in the graphs.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{(}}\sphinxstyleliteralstrong{\sphinxupquote{torch.Tensor}}\sphinxstyleliteralstrong{\sphinxupquote{)}} \textendash{} The weighted adjacency tensor with the dimensions (B x N x N) where B is the batch
size and N is the number of nodes in each graph.

\end{itemize}

\item[{Return (torch.Tensor)}] \leavevmode
The site properties tensor with dimensions (B x N x1) where B is the batch
size and N is the number of nodes in each graph.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.nn.models\_test module}
\label{\detokenize{modules/gqcml.nn:module-gqcml.nn.models_test}}\label{\detokenize{modules/gqcml.nn:gqcml-nn-models-test-module}}\index{module@\spxentry{module}!gqcml.nn.models\_test@\spxentry{gqcml.nn.models\_test}}\index{gqcml.nn.models\_test@\spxentry{gqcml.nn.models\_test}!module@\spxentry{module}}\index{GCNConv\_gqcml (class in gqcml.nn.models\_test)@\spxentry{GCNConv\_gqcml}\spxextra{class in gqcml.nn.models\_test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models_test.GCNConv_gqcml}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.models\_test.}}\sphinxbfcode{\sphinxupquote{GCNConv\_gqcml}}}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.nn.models\_test.GCNConv\_gqcml method)@\spxentry{forward()}\spxextra{gqcml.nn.models\_test.GCNConv\_gqcml method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models_test.GCNConv_gqcml.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{node\_features}}, \emph{\DUrole{n}{adj\_matrices}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{GCNConv\_tg (class in gqcml.nn.models\_test)@\spxentry{GCNConv\_tg}\spxextra{class in gqcml.nn.models\_test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models_test.GCNConv_tg}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.nn.models\_test.}}\sphinxbfcode{\sphinxupquote{GCNConv\_tg}}}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.nn.models\_test.GCNConv\_tg method)@\spxentry{forward()}\spxextra{gqcml.nn.models\_test.GCNConv\_tg method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.nn:gqcml.nn.models_test.GCNConv_tg.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}



\section{Module contents}
\label{\detokenize{modules/gqcml.nn:module-gqcml.nn}}\label{\detokenize{modules/gqcml.nn:module-contents}}\index{module@\spxentry{module}!gqcml.nn@\spxentry{gqcml.nn}}\index{gqcml.nn@\spxentry{gqcml.nn}!module@\spxentry{module}}

\chapter{gqcml.torchgeom\_interface package}
\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-package}}\label{\detokenize{modules/gqcml.torchgeom_interface::doc}}

\section{Submodules}
\label{\detokenize{modules/gqcml.torchgeom_interface:submodules}}

\section{gqcml.torchgeom\_interface.auto\_encoder module}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface.auto_encoder}}\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-auto-encoder-module}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface.auto\_encoder@\spxentry{gqcml.torchgeom\_interface.auto\_encoder}}\index{gqcml.torchgeom\_interface.auto\_encoder@\spxentry{gqcml.torchgeom\_interface.auto\_encoder}!module@\spxentry{module}}\index{GraphAutoEncoder (class in gqcml.torchgeom\_interface.auto\_encoder)@\spxentry{GraphAutoEncoder}\spxextra{class in gqcml.torchgeom\_interface.auto\_encoder}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.GraphAutoEncoder}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.auto\_encoder.}}\sphinxbfcode{\sphinxupquote{GraphAutoEncoder}}}{\emph{\DUrole{n}{num\_aggr}}, \emph{\DUrole{n}{layer\_config}}, \emph{\DUrole{n}{act\_fn}}, \emph{\DUrole{n}{dropout}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{dropout\_rate}\DUrole{o}{=}\DUrole{default_value}{0.1}}, \emph{\DUrole{n}{feature\_transformation}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{decode() (gqcml.torchgeom\_interface.auto\_encoder.GraphAutoEncoder method)@\spxentry{decode()}\spxextra{gqcml.torchgeom\_interface.auto\_encoder.GraphAutoEncoder method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.GraphAutoEncoder.decode}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{decode}}}{\emph{\DUrole{n}{encoded\_node\_features}}}{}
\end{fulllineitems}

\index{encode() (gqcml.torchgeom\_interface.auto\_encoder.GraphAutoEncoder method)@\spxentry{encode()}\spxextra{gqcml.torchgeom\_interface.auto\_encoder.GraphAutoEncoder method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.GraphAutoEncoder.encode}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{encode}}}{\emph{\DUrole{n}{data}}}{}
\end{fulllineitems}

\index{forward() (gqcml.torchgeom\_interface.auto\_encoder.GraphAutoEncoder method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.auto\_encoder.GraphAutoEncoder method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.GraphAutoEncoder.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{Neighbourhood\_Aggregator (class in gqcml.torchgeom\_interface.auto\_encoder)@\spxentry{Neighbourhood\_Aggregator}\spxextra{class in gqcml.torchgeom\_interface.auto\_encoder}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.Neighbourhood_Aggregator}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.auto\_encoder.}}\sphinxbfcode{\sphinxupquote{Neighbourhood\_Aggregator}}}
Bases: \sphinxcode{\sphinxupquote{torch\_geometric.nn.conv.message\_passing.MessagePassing}}

A class that is used in the generation of a feature vector containing
raw node features that are aggregated. This function works in a similar
fashion to GCNConv but removes the use of any bias and weights.
\index{forward() (gqcml.torchgeom\_interface.auto\_encoder.Neighbourhood\_Aggregator method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.auto\_encoder.Neighbourhood\_Aggregator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.Neighbourhood_Aggregator.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{edge\_index}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}torch.Tensor\DUrole{p}{, }torch\_sparse.tensor.SparseTensor\DUrole{p}{{]}}}}, \emph{\DUrole{n}{edge\_weight}\DUrole{p}{:} \DUrole{n}{Optional\DUrole{p}{{[}}torch.Tensor\DUrole{p}{{]}}} \DUrole{o}{=} \DUrole{default_value}{None}}}{{ $\rightarrow$ torch.Tensor}}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{message() (gqcml.torchgeom\_interface.auto\_encoder.Neighbourhood\_Aggregator method)@\spxentry{message()}\spxextra{gqcml.torchgeom\_interface.auto\_encoder.Neighbourhood\_Aggregator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.Neighbourhood_Aggregator.message}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{message}}}{\emph{\DUrole{n}{x\_j}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{edge\_weight}\DUrole{p}{:} \DUrole{n}{Optional\DUrole{p}{{[}}torch.Tensor\DUrole{p}{{]}}}}}{{ $\rightarrow$ torch.Tensor}}
Constructs messages from node \(j\) to node \(i\)
in analogy to \(\phi_{\mathbf{\Theta}}\) for each edge in
\sphinxcode{\sphinxupquote{edge\_index}}.
This function can take any argument as input which was initially
passed to \sphinxcode{\sphinxupquote{propagate()}}.
Furthermore, tensors passed to \sphinxcode{\sphinxupquote{propagate()}} can be mapped to the
respective nodes \(i\) and \(j\) by appending \sphinxcode{\sphinxupquote{\_i}} or
\sphinxcode{\sphinxupquote{\_j}} to the variable name, \sphinxstyleemphasis{.e.g.} \sphinxcode{\sphinxupquote{x\_i}} and \sphinxcode{\sphinxupquote{x\_j}}.

\end{fulllineitems}

\index{message\_and\_aggregate() (gqcml.torchgeom\_interface.auto\_encoder.Neighbourhood\_Aggregator method)@\spxentry{message\_and\_aggregate()}\spxextra{gqcml.torchgeom\_interface.auto\_encoder.Neighbourhood\_Aggregator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.Neighbourhood_Aggregator.message_and_aggregate}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{message\_and\_aggregate}}}{\emph{\DUrole{n}{adj\_t}\DUrole{p}{:} \DUrole{n}{torch\_sparse.tensor.SparseTensor}}, \emph{\DUrole{n}{x}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}}{{ $\rightarrow$ torch.Tensor}}
Fuses computations of {\hyperref[\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.Neighbourhood_Aggregator.message}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{message()}}}}} and \sphinxcode{\sphinxupquote{aggregate()}} into a
single function.
If applicable, this saves both time and memory since messages do not
explicitly need to be materialized.
This function will only gets called in case it is implemented and
propagation takes place based on a \sphinxcode{\sphinxupquote{torch\_sparse.SparseTensor}}.

\end{fulllineitems}


\end{fulllineitems}

\index{sum\_similarity\_loss() (in module gqcml.torchgeom\_interface.auto\_encoder)@\spxentry{sum\_similarity\_loss()}\spxextra{in module gqcml.torchgeom\_interface.auto\_encoder}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.sum_similarity_loss}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.auto\_encoder.}}\sphinxbfcode{\sphinxupquote{sum\_similarity\_loss}}}{\emph{\DUrole{n}{inp\_tensor}}, \emph{\DUrole{n}{decoded\_tensor}}}{}
\end{fulllineitems}

\index{train\_model() (in module gqcml.torchgeom\_interface.auto\_encoder)@\spxentry{train\_model()}\spxextra{in module gqcml.torchgeom\_interface.auto\_encoder}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.auto_encoder.train_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.auto\_encoder.}}\sphinxbfcode{\sphinxupquote{train\_model}}}{\emph{\DUrole{n}{device}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{nmb\_epochs}}, \emph{\DUrole{n}{train\_loader}}, \emph{\DUrole{n}{val\_loader}}, \emph{\DUrole{n}{loss\_fn}}, \emph{\DUrole{n}{optimizer}}, \emph{\DUrole{n}{model\_logger}}, \emph{\DUrole{n}{scheduler}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
train model

A function that trains a given neural network

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param device (str)}] \leavevmode
The device to which the model and tensors are moved during training.

\item[{param model (torch.Module)}] \leavevmode
The neural network that needs to be trained. This is aa torch.nn.Module class that
has a forward function

\item[{param nmb\_epochs (int)}] \leavevmode
The number of epochs that the model needs to be trained

\item[{param train\_loader (torch.DataLoader)}] \leavevmode
The dataloader that contains the training dataset and generates batches

\item[{param val\_loader (torch.DataLoader)}] \leavevmode
The dataloader that contains the validation dataset and generates batches

\item[{param loss\_fn (torch.nn)}] \leavevmode
The loss function that is used during the optimization of the neural network

\item[{param optimizer (torch.optim)}] \leavevmode
The optimizer that is used during the training of the model

\item[{param model\_logger (gqcml.utils.train)}] \leavevmode
The model logger class that registers the training progress and saves the best model

\item[{param (optional) scheduler (torch.optim.lr\_scheduler)}] \leavevmode
A scheduler for decreasing the learning rate

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}



\section{gqcml.torchgeom\_interface.blocks module}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface.blocks}}\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-blocks-module}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface.blocks@\spxentry{gqcml.torchgeom\_interface.blocks}}\index{gqcml.torchgeom\_interface.blocks@\spxentry{gqcml.torchgeom\_interface.blocks}!module@\spxentry{module}}\index{GCNConv\_block (class in gqcml.torchgeom\_interface.blocks)@\spxentry{GCNConv\_block}\spxextra{class in gqcml.torchgeom\_interface.blocks}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.blocks.GCNConv_block}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.blocks.}}\sphinxbfcode{\sphinxupquote{GCNConv\_block}}}{\emph{\DUrole{n}{inp\_dim}}, \emph{\DUrole{n}{hidden\_channels}}, \emph{\DUrole{n}{num\_layers}}, \emph{\DUrole{n}{act\_fn}}, \emph{\DUrole{n}{add\_self\_loops}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{residual}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.torchgeom\_interface.blocks.GCNConv\_block method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.blocks.GCNConv\_block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.blocks.GCNConv_block.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{edge\_idx}}, \emph{\DUrole{n}{edge\_attr}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.blocks.GCNConv\_block method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.blocks.GCNConv\_block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.blocks.GCNConv_block.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{GraphConv\_block (class in gqcml.torchgeom\_interface.blocks)@\spxentry{GraphConv\_block}\spxextra{class in gqcml.torchgeom\_interface.blocks}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.blocks.GraphConv_block}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.blocks.}}\sphinxbfcode{\sphinxupquote{GraphConv\_block}}}{\emph{\DUrole{n}{inp\_dim}}, \emph{\DUrole{n}{hidden\_channels}}, \emph{\DUrole{n}{num\_layers}}, \emph{\DUrole{n}{act\_fn}}, \emph{\DUrole{n}{aggr}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}add\textquotesingle{}}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{residual}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.torchgeom\_interface.blocks.GraphConv\_block method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.blocks.GraphConv\_block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.blocks.GraphConv_block.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{edge\_idx}}, \emph{\DUrole{n}{edge\_attr}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.blocks.GraphConv\_block method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.blocks.GraphConv\_block method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.blocks.GraphConv_block.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.torchgeom\_interface.datasets module}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface.datasets}}\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-datasets-module}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface.datasets@\spxentry{gqcml.torchgeom\_interface.datasets}}\index{gqcml.torchgeom\_interface.datasets@\spxentry{gqcml.torchgeom\_interface.datasets}!module@\spxentry{module}}\index{TriuDataset() (in module gqcml.torchgeom\_interface.datasets)@\spxentry{TriuDataset()}\spxextra{in module gqcml.torchgeom\_interface.datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.datasets.TriuDataset}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.datasets.}}\sphinxbfcode{\sphinxupquote{TriuDataset}}}{\emph{\DUrole{n}{dim}}, \emph{\DUrole{n}{trius}}, \emph{\DUrole{n}{output\_values}}, \emph{\DUrole{n}{residual}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
\end{fulllineitems}

\index{graph\_to\_Data() (in module gqcml.torchgeom\_interface.datasets)@\spxentry{graph\_to\_Data()}\spxextra{in module gqcml.torchgeom\_interface.datasets}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.datasets.graph_to_Data}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.datasets.}}\sphinxbfcode{\sphinxupquote{graph\_to\_Data}}}{\emph{\DUrole{n}{nf}}, \emph{\DUrole{n}{am}}, \emph{\DUrole{n}{output}}}{}
\end{fulllineitems}



\section{gqcml.torchgeom\_interface.models module}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface.models}}\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-models-module}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface.models@\spxentry{gqcml.torchgeom\_interface.models}}\index{gqcml.torchgeom\_interface.models@\spxentry{gqcml.torchgeom\_interface.models}!module@\spxentry{module}}\index{Graph\_Convolution\_attentionpool\_model (class in gqcml.torchgeom\_interface.models)@\spxentry{Graph\_Convolution\_attentionpool\_model}\spxextra{class in gqcml.torchgeom\_interface.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_attentionpool_model}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.models.}}\sphinxbfcode{\sphinxupquote{Graph\_Convolution\_attentionpool\_model}}}{\emph{\DUrole{n}{emb}}, \emph{\DUrole{n}{conv\_block}}, \emph{\DUrole{n}{prop\_dnn}}, \emph{\DUrole{n}{gate\_nn}}, \emph{\DUrole{n}{nn}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{max\_feature}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.torchgeom\_interface.models.Graph\_Convolution\_attentionpool\_model method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.models.Graph\_Convolution\_attentionpool\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_attentionpool_model.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.models.Graph\_Convolution\_attentionpool\_model method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.models.Graph\_Convolution\_attentionpool\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_attentionpool_model.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{Graph\_Convolution\_avgpool\_model (class in gqcml.torchgeom\_interface.models)@\spxentry{Graph\_Convolution\_avgpool\_model}\spxextra{class in gqcml.torchgeom\_interface.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_avgpool_model}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.models.}}\sphinxbfcode{\sphinxupquote{Graph\_Convolution\_avgpool\_model}}}{\emph{\DUrole{n}{emb}}, \emph{\DUrole{n}{conv\_block}}, \emph{\DUrole{n}{prop\_dnn}}, \emph{\DUrole{n}{max\_feature}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.torchgeom\_interface.models.Graph\_Convolution\_avgpool\_model method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.models.Graph\_Convolution\_avgpool\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_avgpool_model.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.models.Graph\_Convolution\_avgpool\_model method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.models.Graph\_Convolution\_avgpool\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_avgpool_model.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{Graph\_Convolution\_model (class in gqcml.torchgeom\_interface.models)@\spxentry{Graph\_Convolution\_model}\spxextra{class in gqcml.torchgeom\_interface.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_model}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.models.}}\sphinxbfcode{\sphinxupquote{Graph\_Convolution\_model}}}{\emph{\DUrole{n}{emb}}, \emph{\DUrole{n}{conv\_block}}, \emph{\DUrole{n}{prop\_dnn}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}
\index{forward() (gqcml.torchgeom\_interface.models.Graph\_Convolution\_model method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.models.Graph\_Convolution\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_model.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Defines the computation performed at every call.

Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.models.Graph\_Convolution\_model method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.models.Graph\_Convolution\_model method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models.Graph_Convolution_model.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.torchgeom\_interface.models\_3 module}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface.models_3}}\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-models-3-module}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface.models\_3@\spxentry{gqcml.torchgeom\_interface.models\_3}}\index{gqcml.torchgeom\_interface.models\_3@\spxentry{gqcml.torchgeom\_interface.models\_3}!module@\spxentry{module}}\index{GCNConv (class in gqcml.torchgeom\_interface.models\_3)@\spxentry{GCNConv}\spxextra{class in gqcml.torchgeom\_interface.models\_3}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GCNConv}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.models\_3.}}\sphinxbfcode{\sphinxupquote{GCNConv}}}{\emph{\DUrole{n}{node\_embedding\_nn}}, \emph{\DUrole{n}{gc\_layers}}, \emph{\DUrole{n}{activation\_function}}, \emph{\DUrole{n}{node\_prop\_nn}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{add\_self\_loops}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{residual}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

GCNConv model

A standard graph convolutional network using the torch geomtric interface
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_sites}} \textendash{} number of sites/atoms in the Hückel model

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node\_embedding\_nn}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}) \textendash{} A torch.nn.Module that processes the initial node features. This torch module
takes a tensor with dimensions (B x N x f)  and should return a tensor with
dimensions (B x N x F) where B is the batch size, N is the number of nodes in
the graphs, f is the initial number of node features and F is the number of node features
associated with each node in the graph. F should be equal to the node\_input\_dim
parameter of the GraphConv layers.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{gcn\_configs}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}) \textendash{} A list containing the dimensions of the consecutive layers of the embedding section of the network
The first value of this list corresponds to the dimensions of the node feature vector and the last
value corresponds to the first node feature dimension of the graph convolutions section.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{activation\_function}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn}}) \textendash{} The activation function that will be used in after each layer in the embedding, graph convolution
and property prediction phase of the network

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node\_prop\_nn}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}) \textendash{} A torch.nn.Module that processes the transformed node features after the application
of the graph convolution operators. The network takes the node features with dimension
(B x N x F) where B is the batch size, N is the number of nodes in the graph and F
the number of features associated with each node in the graphs. The output of the network
is a tensor that has the dimensions (B x N x 1) where the resulting features are
mapped to a single node property.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{normalize}} (\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}) \textendash{} Option to enable the use of the normalized form of the graph convolution

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{bias}} (\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}) \textendash{} Option to add a bias to the layers

\end{itemize}

\end{description}\end{quote}
\index{forward() (gqcml.torchgeom\_interface.models\_3.GCNConv method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.models\_3.GCNConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GCNConv.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Forward

The function to compute the model output.

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param data (torch\_geometric.data.Batch)}] \leavevmode
A Batch object from torch geometric obtained from the loader
which contains x, edge\_index, edge\_attr and batch

\item[{return graph property (torch.tensor)}] \leavevmode
A 2D tensor containing the output values associated with the input graphs

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.models\_3.GCNConv method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.models\_3.GCNConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GCNConv.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
Meta

Returns a dictionary that contains all the network parameters sorted according 3
blocks (embedding, gc and property).

\end{fulllineitems}

\index{node\_properties() (gqcml.torchgeom\_interface.models\_3.GCNConv method)@\spxentry{node\_properties()}\spxextra{gqcml.torchgeom\_interface.models\_3.GCNConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GCNConv.node_properties}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{node\_properties}}}{\emph{\DUrole{n}{data}}}{}
Node properties

Returns the properties of the nodes, i.e. the output of the layer before summing to obtain the
graph property

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param data (torch\_geometric.data.Batch)}] \leavevmode
A Batch object from torch geometric obtained from the loader
which contains x, edge\_index, edge\_attr and batch

\item[{return node properties (torch.tensor)}] \leavevmode
A 2D tensor containing the output values associated with the
nodes of the input graphs

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{GatedGraphConv (class in gqcml.torchgeom\_interface.models\_3)@\spxentry{GatedGraphConv}\spxextra{class in gqcml.torchgeom\_interface.models\_3}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GatedGraphConv}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.models\_3.}}\sphinxbfcode{\sphinxupquote{GatedGraphConv}}}{\emph{\DUrole{n}{node\_embedding\_nn}}, \emph{\DUrole{n}{output\_channels}}, \emph{\DUrole{n}{nmb\_layers}}, \emph{\DUrole{n}{node\_prop\_nn}}, \emph{\DUrole{n}{bias}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

Gated Graph Conv model

A standard graph convolutional network using the torch geomtric interface
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{num\_sites}} \textendash{} number of sites/atoms in the Hückel model

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node\_embedding\_nn}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}) \textendash{} A torch.nn.Module that processes the initial node features. This torch module
takes a tensor with dimensions (B x N x f)  and should return a tensor with
dimensions (B x N x F) where B is the batch size, N is the number of nodes in
the graphs, f is the initial number of node features and F is the number of node features
associated with each node in the graph. F should be equal to the node\_input\_dim
parameter of the GraphConv layers.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{gcn\_configs}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}) \textendash{} A list containing the dimensions of the consecutive layers of the embedding section of the network
The first value of this list corresponds to the dimensions of the node feature vector and the last
value corresponds to the first node feature dimension of the graph convolutions section.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{activation\_function}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn}}) \textendash{} The activation function that will be used in after each layer in the embedding, graph convolution
and property prediction phase of the network

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node\_prop\_nn}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}) \textendash{} A torch.nn.Module that processes the transformed node features after the application
of the graph convolution operators. The network takes the node features with dimension
(B x N x F) where B is the batch size, N is the number of nodes in the graph and F
the number of features associated with each node in the graphs. The output of the network
is a tensor that has the dimensions (B x N x 1) where the resulting features are
mapped to a single node property.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{normalize}} (\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}) \textendash{} Option to enable the use of the normalized form of the graph convolution

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{bias}} (\sphinxstyleliteralemphasis{\sphinxupquote{optional}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}) \textendash{} Option to add a bias to the layers

\end{itemize}

\end{description}\end{quote}
\index{forward() (gqcml.torchgeom\_interface.models\_3.GatedGraphConv method)@\spxentry{forward()}\spxextra{gqcml.torchgeom\_interface.models\_3.GatedGraphConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GatedGraphConv.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{data}}}{}
Forward

The function to compute the model output.

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param data (torch\_geometric.data.Batch)}] \leavevmode
A Batch object from torch geometric obtained from the loader
which contains x, edge\_index, edge\_attr and batch

\item[{return graph property (torch.tensor)}] \leavevmode
A 2D tensor containing the output values associated with the input graphs

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}

\index{meta() (gqcml.torchgeom\_interface.models\_3.GatedGraphConv method)@\spxentry{meta()}\spxextra{gqcml.torchgeom\_interface.models\_3.GatedGraphConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GatedGraphConv.meta}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{meta}}}{}{}
Meta

Returns a dictionary that contains all the network parameters sorted according 3
blocks (embedding, gc and property).

\end{fulllineitems}

\index{node\_properties() (gqcml.torchgeom\_interface.models\_3.GatedGraphConv method)@\spxentry{node\_properties()}\spxextra{gqcml.torchgeom\_interface.models\_3.GatedGraphConv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.models_3.GatedGraphConv.node_properties}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{node\_properties}}}{\emph{\DUrole{n}{data}}}{}
Node properties

Returns the properties of the nodes, i.e. the output of the layer before summing to obtain the
graph property

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param data (torch\_geometric.data.Batch)}] \leavevmode
A Batch object from torch geometric obtained from the loader
which contains x, edge\_index, edge\_attr and batch

\item[{return node properties (torch.tensor)}] \leavevmode
A 2D tensor containing the output values associated with the
nodes of the input graphs

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{gqcml.torchgeom\_interface.utils module}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface.utils}}\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml-torchgeom-interface-utils-module}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface.utils@\spxentry{gqcml.torchgeom\_interface.utils}}\index{gqcml.torchgeom\_interface.utils@\spxentry{gqcml.torchgeom\_interface.utils}!module@\spxentry{module}}\index{evaluate\_model() (in module gqcml.torchgeom\_interface.utils)@\spxentry{evaluate\_model()}\spxextra{in module gqcml.torchgeom\_interface.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.utils.evaluate_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.utils.}}\sphinxbfcode{\sphinxupquote{evaluate\_model}}}{\emph{\DUrole{n}{device}}, \emph{\DUrole{n}{model\_path}}, \emph{\DUrole{n}{error\_path}}, \emph{\DUrole{n}{modelname}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{test\_loader}}, \emph{\DUrole{n}{model\_descr}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\_val.pth\textquotesingle{}}}}{}
\end{fulllineitems}

\index{train\_model() (in module gqcml.torchgeom\_interface.utils)@\spxentry{train\_model()}\spxextra{in module gqcml.torchgeom\_interface.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.torchgeom_interface:gqcml.torchgeom_interface.utils.train_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.torchgeom\_interface.utils.}}\sphinxbfcode{\sphinxupquote{train\_model}}}{\emph{\DUrole{n}{device}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{nmb\_epochs}}, \emph{\DUrole{n}{train\_loader}}, \emph{\DUrole{n}{val\_loader}}, \emph{\DUrole{n}{loss\_fn}}, \emph{\DUrole{n}{optimizer}}, \emph{\DUrole{n}{model\_logger}}, \emph{\DUrole{n}{scheduler}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reduction}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}mean\textquotesingle{}}}}{}
train model

A function that trains a given neural network

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param device (str)}] \leavevmode
The device to which the model and tensors are moved during training.

\item[{param model (torch.Module)}] \leavevmode
The neural network that needs to be trained. This is aa torch.nn.Module class that
has a forward function

\item[{param nmb\_epochs (int)}] \leavevmode
The number of epochs that the model needs to be trained

\item[{param train\_loader (torch.DataLoader)}] \leavevmode
The dataloader that contains the training dataset and generates batches

\item[{param val\_loader (torch.DataLoader)}] \leavevmode
The dataloader that contains the validation dataset and generates batches

\item[{param loss\_fn (torch.nn)}] \leavevmode
The loss function that is used during the optimization of the neural network

\item[{param optimizer (torch.optim)}] \leavevmode
The optimizer that is used during the training of the model

\item[{param model\_logger (gqcml.utils.train)}] \leavevmode
The model logger class that registers the training progress and saves the best model

\item[{param (optional) scheduler (torch.optim.lr\_scheduler)}] \leavevmode
A scheduler for decreasing the learning rate

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}



\section{Module contents}
\label{\detokenize{modules/gqcml.torchgeom_interface:module-gqcml.torchgeom_interface}}\label{\detokenize{modules/gqcml.torchgeom_interface:module-contents}}\index{module@\spxentry{module}!gqcml.torchgeom\_interface@\spxentry{gqcml.torchgeom\_interface}}\index{gqcml.torchgeom\_interface@\spxentry{gqcml.torchgeom\_interface}!module@\spxentry{module}}

\chapter{gqcml.utils package}
\label{\detokenize{modules/gqcml.utils:gqcml-utils-package}}\label{\detokenize{modules/gqcml.utils::doc}}

\section{Submodules}
\label{\detokenize{modules/gqcml.utils:submodules}}

\section{gqcml.utils.test module}
\label{\detokenize{modules/gqcml.utils:module-gqcml.utils.test}}\label{\detokenize{modules/gqcml.utils:gqcml-utils-test-module}}\index{module@\spxentry{module}!gqcml.utils.test@\spxentry{gqcml.utils.test}}\index{gqcml.utils.test@\spxentry{gqcml.utils.test}!module@\spxentry{module}}\index{evaluate\_model() (in module gqcml.utils.test)@\spxentry{evaluate\_model()}\spxextra{in module gqcml.utils.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.test.evaluate_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.utils.test.}}\sphinxbfcode{\sphinxupquote{evaluate\_model}}}{\emph{\DUrole{n}{error\_path}}, \emph{\DUrole{n}{model\_path}}, \emph{\DUrole{n}{modelname}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{test\_loader}}, \emph{\DUrole{n}{device}}, \emph{\DUrole{n}{dataset\_descr}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}test\textquotesingle{}}}, \emph{\DUrole{n}{model\_descr}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\_val.pth\textquotesingle{}}}}{}
\end{fulllineitems}



\section{gqcml.utils.train module}
\label{\detokenize{modules/gqcml.utils:module-gqcml.utils.train}}\label{\detokenize{modules/gqcml.utils:gqcml-utils-train-module}}\index{module@\spxentry{module}!gqcml.utils.train@\spxentry{gqcml.utils.train}}\index{gqcml.utils.train@\spxentry{gqcml.utils.train}!module@\spxentry{module}}\index{model\_logger (class in gqcml.utils.train)@\spxentry{model\_logger}\spxextra{class in gqcml.utils.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.train.model_logger}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gqcml.utils.train.}}\sphinxbfcode{\sphinxupquote{model\_logger}}}{\emph{\DUrole{n}{history\_dir}}, \emph{\DUrole{n}{model\_dir}}, \emph{\DUrole{n}{modelname}}, \emph{\DUrole{n}{loss\_fn}}}{}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{add\_epoch\_metrics() (gqcml.utils.train.model\_logger method)@\spxentry{add\_epoch\_metrics()}\spxextra{gqcml.utils.train.model\_logger method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.train.model_logger.add_epoch_metrics}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_epoch\_metrics}}}{\emph{\DUrole{n}{epoch}}, \emph{\DUrole{n}{training\_losses}}, \emph{\DUrole{n}{nmb\_train\_datapoints}}, \emph{\DUrole{n}{validation\_losses}}, \emph{\DUrole{n}{nmb\_val\_datapoints}}}{}
Add epoch metrics

A function used to write the loss metrics of an epoch to the history file.

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param epoch (int)}] \leavevmode
The number of the epoch the model is currently on

\item[{param training\_losses (list)}] \leavevmode
A list containing all the batch training losses during the current epoch

\item[{param nmb\_train\_datapoints (int)}] \leavevmode
The total number of datapoints in the training dataset

\item[{param validation\_losses (list)}] \leavevmode
A list containing all the batch validation losses during the current epoch

\item[{param nmb\_val\_datapoints (int)}] \leavevmode
The total number of datapoints in the validation dataset

\item[{return}] \leavevmode
Nothing is returned, the average loss for the training and validation set is computed together with
the variances of the batch losses and written to the metrics file

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}

\index{check\_improvement() (gqcml.utils.train.model\_logger method)@\spxentry{check\_improvement()}\spxextra{gqcml.utils.train.model\_logger method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.train.model_logger.check_improvement}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_improvement}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{epoch}}}{}
Check improvement

A function to checkpoint the model during optimization based on the evolution of the validation loss.

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param model (torch.Module)}] \leavevmode
The model object that is currently being optimized. This object needs to have
the state\_dict() function in order to retrieve the parameters.

\item[{param epoch (int)}] \leavevmode
The current epoch of the model optimization.

\item[{return}] \leavevmode
Nothing is returned, the function checks whether the validation loss has decreased and saved the model

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}

\index{summary() (gqcml.utils.train.model\_logger method)@\spxentry{summary()}\spxextra{gqcml.utils.train.model\_logger method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.train.model_logger.summary}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{summary}}}{}{}
A function that returns the optimal state of the model during optimization as a dictionary object

\end{fulllineitems}


\end{fulllineitems}

\index{random\_seed() (in module gqcml.utils.train)@\spxentry{random\_seed()}\spxextra{in module gqcml.utils.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.train.random_seed}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.utils.train.}}\sphinxbfcode{\sphinxupquote{random\_seed}}}{\emph{\DUrole{n}{seed\_value}}, \emph{\DUrole{n}{use\_cuda}}}{}
random seed

Sets the random seed for all the different packages to the same value

\end{fulllineitems}

\index{train\_model() (in module gqcml.utils.train)@\spxentry{train\_model()}\spxextra{in module gqcml.utils.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{modules/gqcml.utils:gqcml.utils.train.train_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{gqcml.utils.train.}}\sphinxbfcode{\sphinxupquote{train\_model}}}{\emph{\DUrole{n}{device}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{nmb\_epochs}}, \emph{\DUrole{n}{train\_loader}}, \emph{\DUrole{n}{val\_loader}}, \emph{\DUrole{n}{loss\_fn}}, \emph{\DUrole{n}{optimizer}}, \emph{\DUrole{n}{model\_logger}}, \emph{\DUrole{n}{scheduler}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
train model

A function that trains a given neural network

Parameters
\begin{quote}
\begin{quote}\begin{description}
\item[{param device (str)}] \leavevmode
The device to which the model and tensors are moved during training.

\item[{param model (torch.Module)}] \leavevmode
The neural network that needs to be trained. This is aa torch.nn.Module class that
has a forward function

\item[{param nmb\_epochs (int)}] \leavevmode
The number of epochs that the model needs to be trained

\item[{param train\_loader (torch.DataLoader)}] \leavevmode
The dataloader that contains the training dataset and generates batches

\item[{param val\_loader (torch.DataLoader)}] \leavevmode
The dataloader that contains the validation dataset and generates batches

\item[{param loss\_fn (torch.nn)}] \leavevmode
The loss function that is used during the optimization of the neural network

\item[{param optimizer (torch.optim)}] \leavevmode
The optimizer that is used during the training of the model

\item[{param model\_logger (gqcml.utils.train)}] \leavevmode
The model logger class that registers the training progress and saves the best model

\item[{param (optional) scheduler (torch.optim.lr\_scheduler)}] \leavevmode
A scheduler for decreasing the learning rate

\end{description}\end{quote}
\end{quote}

\end{fulllineitems}



\section{Module contents}
\label{\detokenize{modules/gqcml.utils:module-gqcml.utils}}\label{\detokenize{modules/gqcml.utils:module-contents}}\index{module@\spxentry{module}!gqcml.utils@\spxentry{gqcml.utils}}\index{gqcml.utils@\spxentry{gqcml.utils}!module@\spxentry{module}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{g}
\item\relax\sphinxstyleindexentry{gqcml.data}\sphinxstyleindexpageref{modules/gqcml.data:\detokenize{module-gqcml.data}}
\item\relax\sphinxstyleindexentry{gqcml.data.Data}\sphinxstyleindexpageref{modules/gqcml.data:\detokenize{module-gqcml.data.Data}}
\item\relax\sphinxstyleindexentry{gqcml.data\_generators}\sphinxstyleindexpageref{modules/gqcml.data_generators:\detokenize{module-gqcml.data_generators}}
\item\relax\sphinxstyleindexentry{gqcml.data\_generators.graph\_sampler}\sphinxstyleindexpageref{modules/gqcml.data_generators:\detokenize{module-gqcml.data_generators.graph_sampler}}
\item\relax\sphinxstyleindexentry{gqcml.data\_generators.Huckel}\sphinxstyleindexpageref{modules/gqcml.data_generators:\detokenize{module-gqcml.data_generators.Huckel}}
\item\relax\sphinxstyleindexentry{gqcml.datasets}\sphinxstyleindexpageref{modules/gqcml.datasets:\detokenize{module-gqcml.datasets}}
\item\relax\sphinxstyleindexentry{gqcml.datasets.Datasets}\sphinxstyleindexpageref{modules/gqcml.datasets:\detokenize{module-gqcml.datasets.Datasets}}
\item\relax\sphinxstyleindexentry{gqcml.nn}\sphinxstyleindexpageref{modules/gqcml.nn:\detokenize{module-gqcml.nn}}
\item\relax\sphinxstyleindexentry{gqcml.nn.blocks}\sphinxstyleindexpageref{modules/gqcml.nn:\detokenize{module-gqcml.nn.blocks}}
\item\relax\sphinxstyleindexentry{gqcml.nn.layers}\sphinxstyleindexpageref{modules/gqcml.nn:\detokenize{module-gqcml.nn.layers}}
\item\relax\sphinxstyleindexentry{gqcml.nn.models}\sphinxstyleindexpageref{modules/gqcml.nn:\detokenize{module-gqcml.nn.models}}
\item\relax\sphinxstyleindexentry{gqcml.nn.models\_test}\sphinxstyleindexpageref{modules/gqcml.nn:\detokenize{module-gqcml.nn.models_test}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface.auto\_encoder}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface.auto_encoder}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface.blocks}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface.blocks}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface.datasets}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface.datasets}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface.models}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface.models}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface.models\_3}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface.models_3}}
\item\relax\sphinxstyleindexentry{gqcml.torchgeom\_interface.utils}\sphinxstyleindexpageref{modules/gqcml.torchgeom_interface:\detokenize{module-gqcml.torchgeom_interface.utils}}
\item\relax\sphinxstyleindexentry{gqcml.utils}\sphinxstyleindexpageref{modules/gqcml.utils:\detokenize{module-gqcml.utils}}
\item\relax\sphinxstyleindexentry{gqcml.utils.test}\sphinxstyleindexpageref{modules/gqcml.utils:\detokenize{module-gqcml.utils.test}}
\item\relax\sphinxstyleindexentry{gqcml.utils.train}\sphinxstyleindexpageref{modules/gqcml.utils:\detokenize{module-gqcml.utils.train}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}